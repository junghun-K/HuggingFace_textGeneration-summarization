{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02cc025f",
   "metadata": {},
   "source": [
    "# Bonus point (beam search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7e87846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search\n",
    "# Referenced from machinelearningmastery.com\n",
    "def beam_search_decoder(data, k):\n",
    "    sequences = [[list(), 0.0]]\n",
    "    # walk over each step in sequence\n",
    "    for row in data:\n",
    "        all_candidates = list()\n",
    "        # expand each current candidate\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            for j in range(len(row)):\n",
    "                candidate = [seq + [j], score - log(row[j])]\n",
    "                all_candidates.append(candidate)\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "        # select k best\n",
    "        sequences = ordered[:k]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e44bda",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7f140315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "maxLength = 30\n",
    "\n",
    "prompt = \"Today I believe we can finally\"\n",
    "k = 20\n",
    "p = 0.95\n",
    "\n",
    "# return Pytorch tensor\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cf9e2f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Greedy Search\n",
    "greedy_output = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=maxLength, \n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "# Beam Search\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=maxLength, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True,\n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "# Top-K\n",
    "# set top_k to 20\n",
    "top_k_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_new_tokens=maxLength, \n",
    "    top_k=k,\n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "# Top-P (nuclear sampling)\n",
    "\n",
    "# set top_k = 20 and set top_p = 0.95\n",
    "top_p_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_new_tokens=maxLength, \n",
    "    top_k=k, \n",
    "    top_p=p, \n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43872ae0",
   "metadata": {},
   "source": [
    "### Print outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "347c6435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Search ['Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n\\nI believe that we can']\n",
      "\n",
      "Beam Search ['Today I believe we can finally get to the bottom of this issue.\\n\\n\"We need to find a way to make sure that we don\\'t get into a situation where we']\n",
      "\n",
      "Top-K ['Today I believe we can finally solve the problems that have been so painfully plaguing the people of the United States.\"\\n\\n\\nThe White House released a video message on Friday to supporters']\n",
      "\n",
      "Top-P ['Today I believe we can finally make the case for global citizenship for all Muslims, and to do so we should call upon governments everywhere to join our cause.\"\\n\\nThe United Nations']\n"
     ]
    }
   ],
   "source": [
    "print(f'Greedy Search',tokenizer.batch_decode(greedy_output[0], skip_special_tokens=True))\n",
    "print(f'\\nBeam Search',tokenizer.batch_decode(beam_output[0], skip_special_tokens=True))\n",
    "print(f'\\nTop-K',tokenizer.batch_decode(top_k_output[0], skip_special_tokens=True))\n",
    "print(f'\\nTop-P',tokenizer.batch_decode(top_p_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1ad82",
   "metadata": {},
   "source": [
    "### Generate Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "b862075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = inputs.input_ids.shape[1]\n",
    "# Greedy\n",
    "greedy_output_scores = model.compute_transition_scores(\n",
    "    greedy_output.sequences, greedy_output.scores, normalize_logits=True\n",
    ")\n",
    "greedy_output_tokens = greedy_output.sequences[:, input_length:]\n",
    "\n",
    "# Beam\n",
    "beam_output_scores = model.compute_transition_scores(\n",
    "    beam_output.sequences, beam_output.scores, normalize_logits=True\n",
    ")\n",
    "beam_output_tokens = beam_output.sequences[:, input_length:]\n",
    "\n",
    "# Top-K\n",
    "top_k_output_scores = model.compute_transition_scores(\n",
    "    top_k_output.sequences, top_k_output.scores, normalize_logits=True\n",
    ")\n",
    "top_k_output_tokens = top_k_output.sequences[:, input_length:]\n",
    "\n",
    "# Top-P\n",
    "top_p_output_scores = model.compute_transition_scores(\n",
    "    top_p_output.sequences, top_p_output.scores, normalize_logits=True\n",
    ")\n",
    "top_p_output_tokens = top_p_output.sequences[:, input_length:]\n",
    "\n",
    "# for tok, score in zip(greedy_output_tokens[0], greedy_output_scores[0]):\n",
    "#     # | token | token string | logits | probability\n",
    "#     print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "c1465312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "def calculate_perplexity_and_likelihood(scores):\n",
    "\n",
    "    likelihood = 0\n",
    "    perplexity = 0  \n",
    "    probabilities = []\n",
    "    \n",
    "    # Logit is normalized already    \n",
    "    for score in scores[0]:\n",
    "        \n",
    "        logit = score.numpy()\n",
    "        \n",
    "        # Based on the document, since logit is normalized prob is simply np.exp(logit)\n",
    "        prob = np.exp(logit)\n",
    "        probabilities.append(prob)\n",
    "        \n",
    "    likelihood = np.sum(np.log(probabilities))\n",
    "    perplexity = np.exp(-likelihood / len(probabilities))\n",
    "\n",
    "    print(probabilities)\n",
    "    \n",
    "    return perplexity, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "90be9554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.065907195, 0.13563468, 0.3508611, 0.19267276, 0.8058686, 0.30816844, 0.50976485, 0.04211929, 0.1285854, 0.14757983, 0.2469583, 0.4106969, 0.423665, 0.9764272, 0.08997929, 0.117223665, 0.26768893, 0.09745569, 0.33263528, 0.9752316, 0.23358475, 0.9987701, 0.27346984, 0.29113737, 0.99947906, 0.112904266, 0.1716018, 0.3090825, 0.21305902, 0.53842753]\n",
      "Greedy perplexity: 0.698\n",
      "Greedy likelihood: 10.77\n",
      "[0.06590721, 0.13563484, 0.0002497108, 0.08885794, 0.05639005, 0.009839361, 4.4703913e-05, 0.0016683919, 4.4980472e-05, 0.000119576616, 0.053483225, 4.907175e-07, 9.743545e-05, 0.0032887398, 0.041840654, 0.06388645, 5.7022603e-06, 0.6852837, 0.05227792, 3.1248132e-06, 0.40888914, 0.10307166, 0.14584419, 1.6543321e-05, 0.06552753, 0.00018465803, 0.000103712075, 1.7930624e-06, 0.0003198056, 0.25082445]\n",
      "Beam perplexity: 0.928\n",
      "Beam likelihood: 2.23\n",
      "[0.02993881, 0.4524195, 0.1066755, 0.21563761, 0.25393128, 0.2911282, 0.06665071, 0.020737877, 0.08946182, 0.99889195, 0.22038542, 0.03828162, 0.7900186, 0.23292466, 0.36683416, 0.9642613, 0.04459961, 0.034344282, 0.99963284, 0.22909795, 0.07600729, 0.997859, 0.048551936, 0.6276163, 0.03446408, 0.022815302, 0.24011931, 0.12739629, 0.035439316, 0.2503175]\n",
      "Top-k perplexity: 0.743\n",
      "Top-k likelihood: 8.91\n",
      "[0.10837476, 0.21062534, 0.06353681, 0.5046898, 0.017684577, 0.021407824, 0.29850864, 0.50871944, 0.017655484, 0.16573273, 0.2602387, 0.050904088, 0.04074729, 0.66511, 0.14603679, 0.029053891, 0.023293147, 0.17353944, 0.020230394, 0.058437623, 0.82340986, 0.033363648, 0.10969143, 0.059665196, 0.20920922, 0.8335667, 1.0, 0.31119436, 0.07663311, 0.75095105]\n",
      "Top-p perplexity: 0.776\n",
      "Top-p likelihood: 7.59\n"
     ]
    }
   ],
   "source": [
    "# calculate the perplexity and likelihood for the greedy output\n",
    "greedy_perplexity, greedy_likelihood = calculate_perplexity_and_likelihood(greedy_output_scores)\n",
    "print(f\"Greedy perplexity: {greedy_perplexity:.3f}\")\n",
    "print(f\"Greedy likelihood: {greedy_likelihood:.2f}\")\n",
    "\n",
    "# calculate the perplexity and likelihood for the beam output\n",
    "beam_perplexity, beam_likelihood = calculate_perplexity_and_likelihood(beam_output_scores)\n",
    "print(f\"Beam perplexity: {beam_perplexity:.3f}\")\n",
    "print(f\"Beam likelihood: {beam_likelihood:.2f}\")\n",
    "\n",
    "# calculate the perplexity and likelihood for the top-k output\n",
    "top_k_perplexity, top_k_likelihood = calculate_perplexity_and_likelihood(top_k_output_scores)\n",
    "print(f\"Top-k perplexity: {top_k_perplexity:.3f}\")\n",
    "print(f\"Top-k likelihood: {top_k_likelihood:.2f}\")\n",
    "\n",
    "# calculate the perplexity and likelihood for the top-p output\n",
    "top_p_perplexity, top_p_likelihood = calculate_perplexity_and_likelihood(top_p_output_scores)\n",
    "print(f\"Top-p perplexity: {top_p_perplexity:.3f}\")\n",
    "print(f\"Top-p likelihood: {top_p_likelihood:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "aa51d07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(greedy_output_scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a4a54800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.70233902655079\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d80dfa",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Samsum: https://huggingface.co/datasets/samsum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae10eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load_dataset(\"samsum\")\n",
    "# !pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e292f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (C:/Users/Jeonghoon Kim/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b6f8e70a5349359901c0aff96a4ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8ec5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"philschmid/bart-large-cnn-samsum\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"philschmid/bart-large-cnn-samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca7ee282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please... be patient...\n",
      "This takes a while...\n",
      "\n",
      "df saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create an empty DataFrame with 5 columns\n",
    "df = pd.DataFrame(columns=['prompt', 'greedy_search', 'beam_search', 'top_k', 'top_p'])\n",
    "\n",
    "\n",
    "# maxLength to 150 to assure full text\n",
    "maxLength = 150\n",
    "train = False\n",
    "\n",
    "if (train):\n",
    "    n = 50\n",
    "else:\n",
    "    n = 0\n",
    "\n",
    "# Create summary from first 50 with test set \n",
    "print('Please... Be patient...\\nThis takes a while...')\n",
    "for i in range(n):\n",
    "    prompt = dataset['test'][i]['dialogue']\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "\n",
    "    # Greedy Search\n",
    "    greedy_output = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=maxLength, \n",
    "        return_dict_in_generate=True, \n",
    "        output_scores=True\n",
    "    )\n",
    "\n",
    "    # Beam Search\n",
    "    beam_output = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=maxLength, \n",
    "        num_beams=5, \n",
    "        no_repeat_ngram_size=2, \n",
    "        early_stopping=True,\n",
    "        return_dict_in_generate=True, \n",
    "        output_scores=True\n",
    "    )\n",
    "\n",
    "    # Top-K\n",
    "    # set top_k to 20\n",
    "    top_k_output = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_new_tokens=maxLength, \n",
    "        top_k=k,\n",
    "        return_dict_in_generate=True, \n",
    "        output_scores=True\n",
    "    )\n",
    "\n",
    "    # Top-P (nuclear sampling)\n",
    "    # set top_k = 20 and set top_p = 0.95\n",
    "    top_p_output = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True, \n",
    "        max_new_tokens=maxLength, \n",
    "        top_k=k, \n",
    "        top_p=p, \n",
    "        return_dict_in_generate=True, \n",
    "        output_scores=True\n",
    "    )\n",
    "    \n",
    "    # convert list to string\n",
    "    greedy = ' '.join(tokenizer.batch_decode(greedy_output[0], skip_special_tokens=True))\n",
    "    beam = ' '.join(tokenizer.batch_decode(beam_output[0], skip_special_tokens=True))\n",
    "    top_k = ' '.join(tokenizer.batch_decode(top_k_output[0], skip_special_tokens=True))\n",
    "    top_p = ' '.join(tokenizer.batch_decode(top_p_output[0], skip_special_tokens=True))\n",
    "    \n",
    "    row = {'prompt': prompt, 'greedy_search': greedy, 'beam_search': beam, 'top_k': top_k, 'top_p': top_p}\n",
    "    df = df.append(row, ignore_index=True)\n",
    "    \n",
    "# save to csv file\n",
    "if (train):\n",
    "    df.to_csv('output.csv', encoding='utf-8-sig', index=False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "print('\\ndf saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb1fad9",
   "metadata": {},
   "source": [
    "# Task 3-1\n",
    "\n",
    "### content overlap metrics: BLEU\n",
    "### model-based metrics:  BERT score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6141d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b4b14a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "--------------------------------------------------\n",
      "Hannah is looking for Betty's number. Larry called her last time they were at the park together. Hannah doesn't know Larry very well. Amanda suggests Hannah to text him. Hannah agrees to text Larry instead.   ...  .  \n"
     ]
    }
   ],
   "source": [
    "print(dataset['test']['summary'][0])\n",
    "print('--------------------------------------------------')\n",
    "print(df['greedy_search'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25dd0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "df = pd.read_csv('output.csv', encoding='utf-8-sig')\n",
    "\n",
    "# Get 50 references\n",
    "references = dataset['test']['summary'][:50]\n",
    "\n",
    "# get all predictions\n",
    "greedy_predictions = list(df['greedy_search'])\n",
    "beam_predictions = list(df['beam_search'])\n",
    "top_k_predictions =  list(df['top_k'])\n",
    "top_p_predictions =  list(df['top_p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294d67e1",
   "metadata": {},
   "source": [
    "## Bert Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12b0f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load evaluate metrics\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "greedy_bert = bertscore.compute(predictions=greedy_predictions, references=references, lang=\"en\")\n",
    "beam_bert = bertscore.compute(predictions=beam_predictions, references=references, lang=\"en\")\n",
    "top_k_bert = bertscore.compute(predictions=top_k_predictions, references=references, lang=\"en\")\n",
    "top_p_bert = bertscore.compute(predictions=top_p_predictions, references=references, lang=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d09e8",
   "metadata": {},
   "source": [
    "## Blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "92ec14a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# load evaluate metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "greedy_list = []\n",
    "beam_list = []\n",
    "top_k_list = []\n",
    "top_p_list = []\n",
    "\n",
    "for i in range(len(greedy_predictions)):\n",
    "    greedy_blue = bleu.compute(predictions=greedy_predictions[i:i+1], references=references[i:i+1])\n",
    "    beam_blue = bleu.compute(predictions=list(beam_predictions[i:i+1]), references=references[i:i+1])\n",
    "    top_k_blue = bleu.compute(predictions=list(top_k_predictions[i:i+1]), references=references[i:i+1])\n",
    "    top_p_blue = bleu.compute(predictions=list(top_p_predictions[i:i+1]), references=references[i:i+1])\n",
    "    \n",
    "    greedy_list.append(greedy_blue['bleu'])\n",
    "    beam_list.append(beam_blue['bleu'])\n",
    "    top_k_list.append(top_k_blue['bleu'])\n",
    "    top_p_list.append(top_p_blue['bleu'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4b40de7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.22306552953519643,\n",
       " 0.26675945012732133,\n",
       " 0.0,\n",
       " 0.12952741640890278,\n",
       " 0.10369816700638208,\n",
       " 0.21672328066436025,\n",
       " 0.1605459569752503,\n",
       " 0.153876603091887,\n",
       " 0.08621434964845645,\n",
       " 0.06819851521871377,\n",
       " 0.11966751775324048,\n",
       " 0.0,\n",
       " 0.15702128402250726,\n",
       " 0.21866340640321882,\n",
       " 0.04465922625585483,\n",
       " 0.11437790076957295,\n",
       " 0.0,\n",
       " 0.1840040987758381,\n",
       " 0.17303353843686656,\n",
       " 0.07906552205912001,\n",
       " 0.1363970304374275,\n",
       " 0.11336958836647044,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25976620289907654,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0824186035805035,\n",
       " 0.2965879078152069,\n",
       " 0.0,\n",
       " 0.06455672843053721,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3381677279168117,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1806702354428209,\n",
       " 0.0,\n",
       " 0.09473323932047992,\n",
       " 0.09212480089422209,\n",
       " 0.1365091799434783,\n",
       " 0.07786956590083421,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bbd0a2",
   "metadata": {},
   "source": [
    "### Add Scores to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e45caac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['greedy_bert_score'] = pd.Series(greedy_bert['f1'])\n",
    "df['beam_bert_score'] = pd.Series(beam_bert['f1'])\n",
    "df['top_k_bert_score'] = pd.Series(top_k_bert['f1'])\n",
    "df['top_p_bert_score'] = pd.Series(top_p_bert['f1'])\n",
    "\n",
    "df['greedy_bleu'] = pd.Series(greedy_list)\n",
    "df['beam_bleu'] = pd.Series(beam_list)\n",
    "df['top_k_bleu'] = pd.Series(top_k_list)\n",
    "df['top_p_bleu'] = pd.Series(top_p_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "23e3df4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df was saved to \" outputWithScores.csv \"\n"
     ]
    }
   ],
   "source": [
    "savePath ='outputWithScores.csv'\n",
    "df.to_csv(savePath, encoding='utf-8-sig', index=False)\n",
    "print('df was saved to \"', savePath, '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c136e1",
   "metadata": {},
   "source": [
    "### Compute Average of each evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b66b5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
