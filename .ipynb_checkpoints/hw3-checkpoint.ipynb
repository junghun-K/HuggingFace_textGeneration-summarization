{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e44bda",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f140315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "maxLength = 30\n",
    "\n",
    "prompt = \"Today I believe we can finally\"\n",
    "k = 20\n",
    "p = 0.95\n",
    "\n",
    "# return Pytorch tensor\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9e2f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Greedy Search\n",
    "greedy_output = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=maxLength, \n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "# Beam Search\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=maxLength, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True,\n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "# Top-K\n",
    "# set top_k to 20\n",
    "top_k_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_new_tokens=maxLength, \n",
    "    top_k=k,\n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "# Top-P (nuclear sampling)\n",
    "\n",
    "# set top_k = 20 and set top_p = 0.95\n",
    "top_p_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_new_tokens=maxLength, \n",
    "    top_k=k, \n",
    "    top_p=p, \n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43872ae0",
   "metadata": {},
   "source": [
    "### Print outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "347c6435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Search ['Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n\\nI believe that we can']\n",
      "\n",
      "Beam Search ['Today I believe we can finally get to the bottom of this issue.\\n\\n\"We need to find a way to make sure that we don\\'t get into a situation where we']\n",
      "\n",
      "Top-K ['Today I believe we can finally start a new chapter in our history.\\n\\nI love people who are dedicated to their craft.\\n\\nI want my kids to be like my']\n",
      "\n",
      "Top-P ['Today I believe we can finally do things better.\"\\n\\nThat includes making sure that all new vehicles meet a set of safety regulations before new ones are launched. This means they must']\n"
     ]
    }
   ],
   "source": [
    "print(f'Greedy Search',tokenizer.batch_decode(greedy_output[0], skip_special_tokens=True))\n",
    "print(f'\\nBeam Search',tokenizer.batch_decode(beam_output[0], skip_special_tokens=True))\n",
    "print(f'\\nTop-K',tokenizer.batch_decode(top_k_output[0], skip_special_tokens=True))\n",
    "print(f'\\nTop-P',tokenizer.batch_decode(top_p_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1ad82",
   "metadata": {},
   "source": [
    "### Generate Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b862075b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1029: UserWarning: Use of index_put_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[indices] = tensor (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:634.)\n",
      "  beam_indices[beam_indices_mask] = 0\n",
      "D:\\Anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1029: UserWarning: Use of masked_fill_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[mask] = scalar (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:1654.)\n",
      "  beam_indices[beam_indices_mask] = 0\n"
     ]
    }
   ],
   "source": [
    "input_length = inputs.input_ids.shape[1]\n",
    "# Greedy\n",
    "greedy_output_scores = model.compute_transition_scores(\n",
    "    greedy_output.sequences, greedy_output.scores, normalize_logits=True\n",
    ")\n",
    "greedy_output_tokens = greedy_output.sequences[:, input_length:]\n",
    "\n",
    "# Beam\n",
    "beam_output_scores = model.compute_transition_scores(\n",
    "    beam_output.sequences, beam_output.scores, normalize_logits=True\n",
    ")\n",
    "beam_output_tokens = beam_output.sequences[:, input_length:]\n",
    "\n",
    "# Top-K\n",
    "top_k_output_scores = model.compute_transition_scores(\n",
    "    top_k_output.sequences, top_k_output.scores, normalize_logits=True\n",
    ")\n",
    "top_k_output_tokens = top_k_output.sequences[:, input_length:]\n",
    "\n",
    "# Top-P\n",
    "top_p_output_scores = model.compute_transition_scores(\n",
    "    top_p_output.sequences, top_p_output.scores, normalize_logits=True\n",
    ")\n",
    "top_p_output_tokens = top_p_output.sequences[:, input_length:]\n",
    "\n",
    "# for tok, score in zip(greedy_output_tokens[0], greedy_output_scores[0]):\n",
    "#     # | token | token string | logits | probability\n",
    "#     print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a97e5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for perplexity and likelihood\n",
    "def calculate_perplexity_and_likelihood(scores):\n",
    "    # compute the sum of the log-likelihoods\n",
    "    perplexity = 0\n",
    "    likelihood = 0\n",
    "    for score in scores[0]:\n",
    "        perplexity += score.numpy()\n",
    "        likelihood += np.exp(perplexity)\n",
    "    perplexity = perplexity / len(scores[0])    \n",
    "    return perplexity, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90be9554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy perplexity: -1.346\n",
      "Greedy likelihood: 7.93%\n",
      "Beam perplexity: -6.135\n",
      "Beam likelihood: 7.48%\n",
      "Top-k perplexity: -1.799\n",
      "Top-k likelihood: 5.79%\n",
      "Top-p perplexity: -2.432\n",
      "Top-p likelihood: 4.73%\n"
     ]
    }
   ],
   "source": [
    "# calculate the perplexity and likelihood for the greedy output\n",
    "greedy_perplexity, greedy_likelihood = calculate_perplexity_and_likelihood(greedy_output_scores)\n",
    "print(f\"Greedy perplexity: {greedy_perplexity:.3f}\")\n",
    "print(f\"Greedy likelihood: {greedy_likelihood:.2%}\")\n",
    "\n",
    "# calculate the perplexity and likelihood for the beam output\n",
    "beam_perplexity, beam_likelihood = calculate_perplexity_and_likelihood(beam_output_scores)\n",
    "print(f\"Beam perplexity: {beam_perplexity:.3f}\")\n",
    "print(f\"Beam likelihood: {beam_likelihood:.2%}\")\n",
    "\n",
    "# calculate the perplexity and likelihood for the top-k output\n",
    "top_k_perplexity, top_k_likelihood = calculate_perplexity_and_likelihood(top_k_output_scores)\n",
    "print(f\"Top-k perplexity: {top_k_perplexity:.3f}\")\n",
    "print(f\"Top-k likelihood: {top_k_likelihood:.2%}\")\n",
    "\n",
    "# calculate the perplexity and likelihood for the top-p output\n",
    "top_p_perplexity, top_p_likelihood = calculate_perplexity_and_likelihood(top_p_output_scores)\n",
    "print(f\"Top-p perplexity: {top_p_perplexity:.3f}\")\n",
    "print(f\"Top-p likelihood: {top_p_likelihood:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d80dfa",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Samsum: https://huggingface.co/datasets/samsum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4195677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load_dataset(\"samsum\")\n",
    "# !pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47e292f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (C:/Users/Jeonghoon Kim/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f124f2bef2471199270cbeafe5a3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8ec5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"philschmid/bart-large-cnn-samsum\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"philschmid/bart-large-cnn-samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca7ee282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please... be patient...\n",
      "This takes a while...\n",
      "\n",
      "df saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create an empty DataFrame with 5 columns\n",
    "df = pd.DataFrame(columns=['prompt', 'greedy_search', 'beam_search', 'top_k', 'top_p'])\n",
    "\n",
    "\n",
    "# maxLength to 150 to assure full text\n",
    "maxLength = 150\n",
    "\n",
    "# Create summary from first 50 with test set \n",
    "print('Please... be patient...\\nThis takes a while...')\n",
    "for i in range(50):\n",
    "    prompt = dataset['test'][i]['dialogue']\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "\n",
    "    # Greedy Search\n",
    "    greedy_output = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=maxLength, \n",
    "        return_dict_in_generate=True, \n",
    "        output_scores=True\n",
    "    )\n",
    "\n",
    "    # Beam Search\n",
    "    beam_output = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=maxLength, \n",
    "        num_beams=5, \n",
    "        no_repeat_ngram_size=2, \n",
    "        early_stopping=True,\n",
    "        return_dict_in_generate=True, \n",
    "        output_scores=True\n",
    "    )\n",
    "\n",
    "    # Top-K\n",
    "    # set top_k to 20\n",
    "    top_k_output = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_new_tokens=maxLength, \n",
    "        top_k=k,\n",
    "        return_dict_in_generate=True, \n",
    "        output_scores=True\n",
    "    )\n",
    "\n",
    "    # Top-P (nuclear sampling)\n",
    "    # set top_k = 20 and set top_p = 0.95\n",
    "    top_p_output = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True, \n",
    "        max_new_tokens=maxLength, \n",
    "        top_k=k, \n",
    "        top_p=p, \n",
    "        return_dict_in_generate=True, \n",
    "        output_scores=True\n",
    "    )\n",
    "    \n",
    "    # convert list to string\n",
    "    greedy = ' '.join(tokenizer.batch_decode(greedy_output[0], skip_special_tokens=True))\n",
    "    beam = ' '.join(tokenizer.batch_decode(beam_output[0], skip_special_tokens=True))\n",
    "    top_k = ' '.join(tokenizer.batch_decode(top_k_output[0], skip_special_tokens=True))\n",
    "    top_p = ' '.join(tokenizer.batch_decode(top_p_output[0], skip_special_tokens=True))\n",
    "    \n",
    "    row = {'prompt': prompt, 'greedy_search': greedy, 'beam_search': beam, 'top_k': top_k, 'top_p': top_p}\n",
    "    df = df.append(row, ignore_index=True)\n",
    "    \n",
    "# save to csv file\n",
    "df.to_csv('output.csv', encoding='utf-8-sig', index=False)\n",
    "print('\\ndf saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dac3fc7",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81c420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
