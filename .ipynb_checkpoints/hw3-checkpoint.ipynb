{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f140315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "maxLength = 20\n",
    "\n",
    "prompt = \"Today I believe we can finally\"\n",
    "\n",
    "# return Pytorch tensor\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf9e2f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Greedy Search\n",
    "greedy_output = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=maxLength, \n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "# Beam Search\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=maxLength, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True,\n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "# Top-K\n",
    "# set top_k to 50\n",
    "top_k_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_new_tokens=maxLength, \n",
    "    top_k=50,\n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "# Top-P (nuclear sampling)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "top_p_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_new_tokens=maxLength, \n",
    "    top_k=50, \n",
    "    top_p=0.95, \n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e32f4",
   "metadata": {},
   "source": [
    "### Print outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d95e6b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Search ['Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States']\n",
      "\n",
      "Beam Search ['Today I believe we can finally get to the bottom of this issue.\\n\\n\"We need to find a way to make sure']\n",
      "\n",
      "Top-K ['Today I believe we can finally do what\\'s right for our country\" and also \"take a stand against all forms of violence and']\n",
      "\n",
      "Top-P ['Today I believe we can finally build an inclusive climate in the UK. I want all of us to have something to celebrate by coming']\n"
     ]
    }
   ],
   "source": [
    "print(f'Greedy Search',tokenizer.batch_decode(greedy_output[0], skip_special_tokens=True))\n",
    "print(f'\\nBeam Search',tokenizer.batch_decode(beam_output[0], skip_special_tokens=True))\n",
    "print(f'\\nTop-K',tokenizer.batch_decode(top_k_output[0], skip_special_tokens=True))\n",
    "print(f'\\nTop-P',tokenizer.batch_decode(top_p_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d6facf",
   "metadata": {},
   "source": [
    "### Generate Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e4219",
   "metadata": {},
   "source": [
    "greedy_output  \n",
    "beam_output  \n",
    "top_k_output  \n",
    "top_p_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b862075b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   651 |  get     | -2.720 | 6.59%\n",
      "|   284 |  to      | -1.998 | 13.56%\n",
      "|   262 |  the     | -1.047 | 35.09%\n",
      "|   966 |  point   | -1.647 | 19.27%\n",
      "|   810 |  where   | -0.216 | 80.59%\n",
      "|   356 |  we      | -1.177 | 30.82%\n",
      "|   460 |  can     | -0.674 | 50.98%\n",
      "|   787 |  make    | -3.167 | 4.21%\n",
      "|   257 |  a       | -2.051 | 12.86%\n",
      "|  3580 |  difference | -1.913 | 14.76%\n",
      "|   287 |  in      | -1.399 | 24.70%\n",
      "|   262 |  the     | -0.890 | 41.07%\n",
      "|  3160 |  lives   | -0.859 | 42.37%\n",
      "|   286 |  of      | -0.024 | 97.64%\n",
      "|   262 |  the     | -2.408 | 9.00%\n",
      "|   661 |  people  | -2.144 | 11.72%\n",
      "|   286 |  of      | -1.318 | 26.77%\n",
      "|   262 |  the     | -2.328 | 9.75%\n",
      "|  1578 |  United  | -1.101 | 33.26%\n",
      "|  1829 |  States  | -0.025 | 97.52%\n"
     ]
    }
   ],
   "source": [
    "input_length = inputs.input_ids.shape[1]\n",
    "# Greedy\n",
    "greedy_output_scores = model.compute_transition_scores(\n",
    "    greedy_output.sequences, greedy_output.scores, normalize_logits=True\n",
    ")\n",
    "greedy_output_tokens = greedy_output.sequences[:, input_length:]\n",
    "\n",
    "# Beam\n",
    "beam_output_scores = model.compute_transition_scores(\n",
    "    beam_output.sequences, beam_output.scores, normalize_logits=True\n",
    ")\n",
    "beam_output_tokens = beam_output.sequences[:, input_length:]\n",
    "\n",
    "# Top-K\n",
    "top_k_output_scores = model.compute_transition_scores(\n",
    "    top_k_output.sequences, top_k_output.scores, normalize_logits=True\n",
    ")\n",
    "top_k_output_tokens = top_k_output.sequences[:, input_length:]\n",
    "\n",
    "# Top-P\n",
    "top_p_output_scores = model.compute_transition_scores(\n",
    "    top_p_output.sequences, top_p_output.scores, normalize_logits=True\n",
    ")\n",
    "top_p_output_tokens = top_p_output.sequences[:, input_length:]\n",
    "\n",
    "# for tok, score in zip(greedy_output_tokens[0], greedy_output_scores[0]):\n",
    "#     # | token | token string | logits | probability\n",
    "#     print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d056a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "\n",
    "def calculate_perplexity_and_likelihood(scores):\n",
    "    # compute the sum of the log-likelihoods\n",
    "    perplexity = 0\n",
    "    likelihood = 0\n",
    "    for score in scores[0]:\n",
    "        perplexity += score.numpy()\n",
    "        likelihood += np.exp(perplexity)\n",
    "    perplexity = perplexity / len(scores[0])    \n",
    "    return perplexity, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f68e632a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy perplexity: -1.455\n",
      "Greedy likelihood: 7.93%\n",
      "Beam perplexity: -6.240\n",
      "Beam likelihood: 7.48%\n",
      "Top-k perplexity: -2.153\n",
      "Top-k likelihood: 3.38%\n",
      "Top-p perplexity: -2.642\n",
      "Top-p likelihood: 2.55%\n"
     ]
    }
   ],
   "source": [
    "# calculate the perplexity and likelihood for the greedy output\n",
    "greedy_perplexity, greedy_likelihood = calculate_perplexity_and_likelihood(greedy_output_scores)\n",
    "print(f\"Greedy perplexity: {greedy_perplexity:.3f}\")\n",
    "print(f\"Greedy likelihood: {greedy_likelihood:.2%}\")\n",
    "\n",
    "# calculate the perplexity and likelihood for the beam output\n",
    "beam_perplexity, beam_likelihood = calculate_perplexity_and_likelihood(beam_output_scores)\n",
    "print(f\"Beam perplexity: {beam_perplexity:.3f}\")\n",
    "print(f\"Beam likelihood: {beam_likelihood:.2%}\")\n",
    "\n",
    "# calculate the perplexity and likelihood for the top-k output\n",
    "top_k_perplexity, top_k_likelihood = calculate_perplexity_and_likelihood(top_k_output_scores)\n",
    "print(f\"Top-k perplexity: {top_k_perplexity:.3f}\")\n",
    "print(f\"Top-k likelihood: {top_k_likelihood:.2%}\")\n",
    "\n",
    "# calculate the perplexity and likelihood for the top-p output\n",
    "top_p_perplexity, top_p_likelihood = calculate_perplexity_and_likelihood(top_p_output_scores)\n",
    "print(f\"Top-p perplexity: {top_p_perplexity:.3f}\")\n",
    "print(f\"Top-p likelihood: {top_p_likelihood:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec253fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97899082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e4478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
