{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02cc025f",
   "metadata": {},
   "source": [
    "# Bonus point (beam search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "7e87846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search\n",
    "# Referenced from machinelearningmastery.com\n",
    "def beam_search_decoder(data, k):\n",
    "    sequences = [[list(), 0.0]]\n",
    "    # walk over each step in sequence\n",
    "    for row in data:\n",
    "        all_candidates = list()\n",
    "        # expand each current candidate\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            for j in range(len(row)):\n",
    "                candidate = [seq + [j], score - log(row[j])]\n",
    "                all_candidates.append(candidate)\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "        # select k best\n",
    "        sequences = ordered[:k]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e44bda",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "7f140315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "maxLength = 30\n",
    "\n",
    "prompt = \"Today I believe we can finally,\"\n",
    "k = 20\n",
    "p = 0.95\n",
    "\n",
    "# return Pytorch tensor\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "cf9e2f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Greedy Search\n",
    "greedy_output = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=maxLength, \n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "# Beam Search\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=maxLength, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True,\n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "# Top-K\n",
    "# set top_k to 20\n",
    "top_k_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_new_tokens=maxLength, \n",
    "    top_k=k,\n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "# Top-P (nuclear sampling)\n",
    "\n",
    "# set top_k = 20 and set top_p = 0.95\n",
    "top_p_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_new_tokens=maxLength, \n",
    "    top_k=k, \n",
    "    top_p=p, \n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43872ae0",
   "metadata": {},
   "source": [
    "### Print outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "347c6435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Search ['Today I believe we can finally, and I believe we can finally, get to the point where we can make a difference in the lives of people who are struggling with mental illness.\\n']\n",
      "\n",
      "Beam Search [\"Today I believe we can finally, at last, get to the bottom of what's going on in this country.\\n\\nI think it's important for us all to understand that we\"]\n",
      "\n",
      "Top-K [\"Today I believe we can finally, at some point in our lives, start taking responsibility for our own actions. It's been my experience since my teenage years that a good lot of people\"]\n",
      "\n",
      "Top-P ['Today I believe we can finally, with our new platform, reach our full potential.\\n\\nIt is now your turn to get involved. We are working on the final version. Please']\n"
     ]
    }
   ],
   "source": [
    "print(f'Greedy Search',tokenizer.batch_decode(greedy_output[0], skip_special_tokens=True))\n",
    "print(f'\\nBeam Search',tokenizer.batch_decode(beam_output[0], skip_special_tokens=True))\n",
    "print(f'\\nTop-K',tokenizer.batch_decode(top_k_output[0], skip_special_tokens=True))\n",
    "print(f'\\nTop-P',tokenizer.batch_decode(top_p_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1ad82",
   "metadata": {},
   "source": [
    "### Generate Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "b862075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = inputs.input_ids.shape[1]\n",
    "# Greedy\n",
    "greedy_output_scores = model.compute_transition_scores(\n",
    "    greedy_output.sequences, greedy_output.scores, normalize_logits=True\n",
    ")\n",
    "greedy_output_tokens = greedy_output.sequences[:, input_length:]\n",
    "\n",
    "# Beam\n",
    "beam_output_scores = model.compute_transition_scores(\n",
    "    beam_output.sequences, beam_output.scores, beam_output.beam_indices, normalize_logits=True\n",
    ")\n",
    "beam_output_tokens = beam_output.sequences[:, input_length:]\n",
    "\n",
    "# Top-K\n",
    "top_k_output_scores = model.compute_transition_scores(\n",
    "    top_k_output.sequences, top_k_output.scores, normalize_logits=True\n",
    ")\n",
    "top_k_output_tokens = top_k_output.sequences[:, input_length:]\n",
    "\n",
    "# Top-P\n",
    "top_p_output_scores = model.compute_transition_scores(\n",
    "    top_p_output.sequences, top_p_output.scores, normalize_logits=True\n",
    ")\n",
    "top_p_output_tokens = top_p_output.sequences[:, input_length:]\n",
    "\n",
    "# for tok, score in zip(greedy_output_tokens[0], greedy_output_scores[0]):\n",
    "#     # | token | token string | logits | probability\n",
    "#     print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "bcdddbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "def calculate_perplexity_and_likelihood(scores):\n",
    "\n",
    "    likelihood = 0\n",
    "    perplexity = 0  \n",
    "    probabilities = []\n",
    "    \n",
    "    # Logit is normalized already    \n",
    "    for score in scores[0]:\n",
    "        \n",
    "        logit = score.numpy()\n",
    "        \n",
    "        # Based on the document, since logit is normalized prob is simply np.exp(logit)\n",
    "        prob = np.exp(logit)\n",
    "        probabilities.append(prob)\n",
    "        \n",
    "    likelihood = np.sum(np.log(probabilities))\n",
    "    perplexity = np.exp(-likelihood / len(probabilities))\n",
    "    \n",
    "    return perplexity, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "90be9554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy perplexity: 4.431\n",
      "Greedy likelihood: -44.66\n",
      "Beam perplexity: 4.204\n",
      "Beam likelihood: -43.08\n",
      "Top-k perplexity: 6.740\n",
      "Top-k likelihood: -57.24\n",
      "Top-p perplexity: 7.121\n",
      "Top-p likelihood: -58.89\n"
     ]
    }
   ],
   "source": [
    "# calculate the perplexity and likelihood for the greedy output\n",
    "greedy_perplexity, greedy_likelihood = calculate_perplexity_and_likelihood(greedy_output_scores)\n",
    "print(f\"Greedy perplexity: {greedy_perplexity:.3f}\")\n",
    "print(f\"Greedy likelihood: {greedy_likelihood:.2f}\")\n",
    "\n",
    "# calculate the perplexity and likelihood for the beam output\n",
    "beam_perplexity, beam_likelihood = calculate_perplexity_and_likelihood(beam_output_scores)\n",
    "print(f\"Beam perplexity: {beam_perplexity:.3f}\")\n",
    "print(f\"Beam likelihood: {beam_likelihood:.2f}\")\n",
    "\n",
    "# calculate the perplexity and likelihood for the top-k output\n",
    "top_k_perplexity, top_k_likelihood = calculate_perplexity_and_likelihood(top_k_output_scores)\n",
    "print(f\"Top-k perplexity: {top_k_perplexity:.3f}\")\n",
    "print(f\"Top-k likelihood: {top_k_likelihood:.2f}\")\n",
    "\n",
    "# calculate the perplexity and likelihood for the top-p output\n",
    "top_p_perplexity, top_p_likelihood = calculate_perplexity_and_likelihood(top_p_output_scores)\n",
    "print(f\"Top-p perplexity: {top_p_perplexity:.3f}\")\n",
    "print(f\"Top-p likelihood: {top_p_likelihood:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d80dfa",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Samsum: https://huggingface.co/datasets/samsum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae10eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load_dataset(\"samsum\")\n",
    "# !pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e292f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (C:/Users/Jeonghoon Kim/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b6f8e70a5349359901c0aff96a4ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8ec5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"philschmid/bart-large-cnn-samsum\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"philschmid/bart-large-cnn-samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca7ee282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please... be patient...\n",
      "This takes a while...\n",
      "\n",
      "df saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create an empty DataFrame with 5 columns\n",
    "df = pd.DataFrame(columns=['prompt', 'greedy_search', 'beam_search', 'top_k', 'top_p'])\n",
    "\n",
    "\n",
    "# maxLength to 150 to assure full text\n",
    "maxLength = 150\n",
    "train = False\n",
    "\n",
    "if (train):\n",
    "    n = 50\n",
    "else:\n",
    "    n = 0\n",
    "\n",
    "# Create summary from first 50 with test set \n",
    "print('Please... Be patient...\\nThis takes a while...')\n",
    "for i in range(n):\n",
    "    prompt = dataset['test'][i]['dialogue']\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "\n",
    "    # Greedy Search\n",
    "    greedy_output = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=maxLength, \n",
    "        return_dict_in_generate=True, \n",
    "        output_scores=True\n",
    "    )\n",
    "\n",
    "    # Beam Search\n",
    "    beam_output = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=maxLength, \n",
    "        num_beams=5, \n",
    "        no_repeat_ngram_size=2, \n",
    "        early_stopping=True,\n",
    "        return_dict_in_generate=True, \n",
    "        output_scores=True\n",
    "    )\n",
    "\n",
    "    # Top-K\n",
    "    # set top_k to 20\n",
    "    top_k_output = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_new_tokens=maxLength, \n",
    "        top_k=k,\n",
    "        return_dict_in_generate=True, \n",
    "        output_scores=True\n",
    "    )\n",
    "\n",
    "    # Top-P (nuclear sampling)\n",
    "    # set top_k = 20 and set top_p = 0.95\n",
    "    top_p_output = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True, \n",
    "        max_new_tokens=maxLength, \n",
    "        top_k=k, \n",
    "        top_p=p, \n",
    "        return_dict_in_generate=True, \n",
    "        output_scores=True\n",
    "    )\n",
    "    \n",
    "    # convert list to string\n",
    "    greedy = ' '.join(tokenizer.batch_decode(greedy_output[0], skip_special_tokens=True))\n",
    "    beam = ' '.join(tokenizer.batch_decode(beam_output[0], skip_special_tokens=True))\n",
    "    top_k = ' '.join(tokenizer.batch_decode(top_k_output[0], skip_special_tokens=True))\n",
    "    top_p = ' '.join(tokenizer.batch_decode(top_p_output[0], skip_special_tokens=True))\n",
    "    \n",
    "    row = {'prompt': prompt, 'greedy_search': greedy, 'beam_search': beam, 'top_k': top_k, 'top_p': top_p}\n",
    "    df = df.append(row, ignore_index=True)\n",
    "    \n",
    "# save to csv file\n",
    "if (train):\n",
    "    df.to_csv('output.csv', encoding='utf-8-sig', index=False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "print('\\ndf saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb1fad9",
   "metadata": {},
   "source": [
    "# Task 3-1\n",
    "\n",
    "### content overlap metrics: BLEU\n",
    "### model-based metrics:  BERT score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3381c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fef0c0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "--------------------------------------------------\n",
      "Hannah is looking for Betty's number. Larry called her last time they were at the park together. Hannah doesn't know Larry very well. Amanda suggests Hannah to text him. Hannah agrees to text Larry instead.   ...  .  \n"
     ]
    }
   ],
   "source": [
    "print(dataset['test']['summary'][0])\n",
    "print('--------------------------------------------------')\n",
    "print(df['greedy_search'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "0aca2b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "df = pd.read_csv('output.csv', encoding='utf-8-sig')\n",
    "\n",
    "# Get 50 references\n",
    "references = dataset['test']['summary'][:50]\n",
    "\n",
    "# get all predictions\n",
    "greedy_predictions = list(df['greedy_search'])\n",
    "beam_predictions = list(df['beam_search'])\n",
    "top_k_predictions =  list(df['top_k'])\n",
    "top_p_predictions =  list(df['top_p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c551517",
   "metadata": {},
   "source": [
    "## Bert Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12b0f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load evaluate metrics\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "greedy_bert = bertscore.compute(predictions=greedy_predictions, references=references, lang=\"en\")\n",
    "beam_bert = bertscore.compute(predictions=beam_predictions, references=references, lang=\"en\")\n",
    "top_k_bert = bertscore.compute(predictions=top_k_predictions, references=references, lang=\"en\")\n",
    "top_p_bert = bertscore.compute(predictions=top_p_predictions, references=references, lang=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e597bc19",
   "metadata": {},
   "source": [
    "## Blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bcc44c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# load evaluate metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "greedy_list = []\n",
    "beam_list = []\n",
    "top_k_list = []\n",
    "top_p_list = []\n",
    "\n",
    "for i in range(len(greedy_predictions)):\n",
    "    greedy_blue = bleu.compute(predictions=greedy_predictions[i:i+1], references=references[i:i+1])\n",
    "    beam_blue = bleu.compute(predictions=list(beam_predictions[i:i+1]), references=references[i:i+1])\n",
    "    top_k_blue = bleu.compute(predictions=list(top_k_predictions[i:i+1]), references=references[i:i+1])\n",
    "    top_p_blue = bleu.compute(predictions=list(top_p_predictions[i:i+1]), references=references[i:i+1])\n",
    "    \n",
    "    greedy_list.append(greedy_blue['bleu'])\n",
    "    beam_list.append(beam_blue['bleu'])\n",
    "    top_k_list.append(top_k_blue['bleu'])\n",
    "    top_p_list.append(top_p_blue['bleu'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b8bb2d",
   "metadata": {},
   "source": [
    "### Add Scores to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cbdf1646",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['greedy_bert_score'] = pd.Series(greedy_bert['f1'])\n",
    "df['beam_bert_score'] = pd.Series(beam_bert['f1'])\n",
    "df['top_k_bert_score'] = pd.Series(top_k_bert['f1'])\n",
    "df['top_p_bert_score'] = pd.Series(top_p_bert['f1'])\n",
    "\n",
    "df['greedy_bleu'] = pd.Series(greedy_list)\n",
    "df['beam_bleu'] = pd.Series(beam_list)\n",
    "df['top_k_bleu'] = pd.Series(top_k_list)\n",
    "df['top_p_bleu'] = pd.Series(top_p_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3410895b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df was saved to \" outputWithScores.csv \"\n"
     ]
    }
   ],
   "source": [
    "savePath ='outputWithAutomaticEvaluation.csv'\n",
    "df.to_csv(savePath, encoding='utf-8-sig', index=False)\n",
    "print('df was saved to \"', savePath, '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee3f30",
   "metadata": {},
   "source": [
    "### Compute Average of each evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "20d41690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Mike took his car into garage today. Ernest is relieved as someone had just crashed into a red Honda which looks like Mike's. \""
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']['summary'][19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "24551ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of BERT score (f1):\n",
      "\n",
      "Greedy search: 89.91%\n",
      "Beam search: 89.85%\n",
      "Top-k sampling: 89.96%\n",
      "Top-p sampling: 90.07%\n",
      "\n",
      "\n",
      "Average of BLEU score:\n",
      "\n",
      "Greedy search: 8.74%\n",
      "Beam search: 7.64%\n",
      "Top-k sampling: 9.28%\n",
      "Top-p sampling: 9.97%\n"
     ]
    }
   ],
   "source": [
    "# compute bert average\n",
    "average_greedy_bert = sum(greedy_bert['f1']) / len(greedy_bert['f1'])\n",
    "average_beam_bert = sum(beam_bert['f1']) / len(beam_bert['f1'])\n",
    "average_top_k_bert = sum(top_k_bert['f1']) / len(top_k_bert['f1'])\n",
    "average_top_p_bert = sum(top_p_bert['f1']) / len(top_p_bert['f1'])\n",
    "\n",
    "# compute bleu average\n",
    "average_greedy_bleu = sum(greedy_list) / len(greedy_list)\n",
    "average_beam_bleu = sum(beam_list) / len(beam_list)\n",
    "average_top_k_bleu = sum(top_k_list) / len(top_k_list)\n",
    "average_top_p_bleu = sum(top_p_list) / len(top_p_list)\n",
    "\n",
    "print('Average of BERT score (f1):\\n')\n",
    "print(f\"Greedy search: {average_greedy_bert:.2%}\")\n",
    "print(f\"Beam search: {average_beam_bert:.2%}\")\n",
    "print(f\"Top-k sampling: {average_top_k_bert:.2%}\")\n",
    "print(f\"Top-p sampling: {average_top_p_bert:.2%}\")\n",
    "\n",
    "print('\\n\\nAverage of BLEU score:\\n')\n",
    "print(f\"Greedy search: {average_greedy_bleu:.2%}\")\n",
    "print(f\"Beam search: {average_beam_bleu:.2%}\")\n",
    "print(f\"Top-k sampling: {average_top_k_bleu:.2%}\")\n",
    "print(f\"Top-p sampling: {average_top_p_bleu:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54d965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
