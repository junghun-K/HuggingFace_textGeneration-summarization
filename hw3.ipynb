{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f140315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "maxLength = 30\n",
    "\n",
    "prompt = \"Today I believe we can finally\"\n",
    "\n",
    "# return Pytorch tensor\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf9e2f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Greedy Search\n",
    "greedy_output = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=maxLength, \n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "# Beam Search\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=maxLength, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True,\n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "# Top-K\n",
    "# set top_k to 50\n",
    "top_k_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_new_tokens=maxLength, \n",
    "    top_k=50,\n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "# Top-P (nuclear sampling)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "top_p_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_new_tokens=maxLength, \n",
    "    top_k=50, \n",
    "    top_p=0.95, \n",
    "    return_dict_in_generate=True, \n",
    "    output_scores=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b1249",
   "metadata": {},
   "source": [
    "### Print outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41f74b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Search ['Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n\\nI believe that we can']\n",
      "\n",
      "Beam Search ['Today I believe we can finally get to the bottom of this issue.\\n\\n\"We need to find a way to make sure that we don\\'t get into a situation where we']\n",
      "\n",
      "Top-K ['Today I believe we can finally make the change for the worse.\\n\\nI am grateful to every single person who has helped make this possible. I hope and expect I can do']\n",
      "\n",
      "Top-P ['Today I believe we can finally achieve the full realization of our vision and our desire to reach the ideal of humanity for our planet,\" said U.N. spokesman Stephane Dujar']\n"
     ]
    }
   ],
   "source": [
    "print(f'Greedy Search',tokenizer.batch_decode(greedy_output[0], skip_special_tokens=True))\n",
    "print(f'\\nBeam Search',tokenizer.batch_decode(beam_output[0], skip_special_tokens=True))\n",
    "print(f'\\nTop-K',tokenizer.batch_decode(top_k_output[0], skip_special_tokens=True))\n",
    "print(f'\\nTop-P',tokenizer.batch_decode(top_p_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dfa60d",
   "metadata": {},
   "source": [
    "### Generate Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b862075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = inputs.input_ids.shape[1]\n",
    "# Greedy\n",
    "greedy_output_scores = model.compute_transition_scores(\n",
    "    greedy_output.sequences, greedy_output.scores, normalize_logits=True\n",
    ")\n",
    "greedy_output_tokens = greedy_output.sequences[:, input_length:]\n",
    "\n",
    "# Beam\n",
    "beam_output_scores = model.compute_transition_scores(\n",
    "    beam_output.sequences, beam_output.scores, normalize_logits=True\n",
    ")\n",
    "beam_output_tokens = beam_output.sequences[:, input_length:]\n",
    "\n",
    "# Top-K\n",
    "top_k_output_scores = model.compute_transition_scores(\n",
    "    top_k_output.sequences, top_k_output.scores, normalize_logits=True\n",
    ")\n",
    "top_k_output_tokens = top_k_output.sequences[:, input_length:]\n",
    "\n",
    "# Top-P\n",
    "top_p_output_scores = model.compute_transition_scores(\n",
    "    top_p_output.sequences, top_p_output.scores, normalize_logits=True\n",
    ")\n",
    "top_p_output_tokens = top_p_output.sequences[:, input_length:]\n",
    "\n",
    "# for tok, score in zip(greedy_output_tokens[0], greedy_output_scores[0]):\n",
    "#     # | token | token string | logits | probability\n",
    "#     print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1db0e04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for perplexity and likelihood\n",
    "def calculate_perplexity_and_likelihood(scores):\n",
    "    # compute the sum of the log-likelihoods\n",
    "    perplexity = 0\n",
    "    likelihood = 0\n",
    "    for score in scores[0]:\n",
    "        perplexity += score.numpy()\n",
    "        likelihood += np.exp(perplexity)\n",
    "    perplexity = perplexity / len(scores[0])    \n",
    "    return perplexity, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef155651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy perplexity: -1.346\n",
      "Greedy likelihood: 7.93%\n",
      "Beam perplexity: -6.135\n",
      "Beam likelihood: 7.48%\n",
      "Top-k perplexity: -2.134\n",
      "Top-k likelihood: 9.00%\n",
      "Top-p perplexity: -2.111\n",
      "Top-p likelihood: 3.69%\n"
     ]
    }
   ],
   "source": [
    "# calculate the perplexity and likelihood for the greedy output\n",
    "greedy_perplexity, greedy_likelihood = calculate_perplexity_and_likelihood(greedy_output_scores)\n",
    "print(f\"Greedy perplexity: {greedy_perplexity:.3f}\")\n",
    "print(f\"Greedy likelihood: {greedy_likelihood:.2%}\")\n",
    "\n",
    "# calculate the perplexity and likelihood for the beam output\n",
    "beam_perplexity, beam_likelihood = calculate_perplexity_and_likelihood(beam_output_scores)\n",
    "print(f\"Beam perplexity: {beam_perplexity:.3f}\")\n",
    "print(f\"Beam likelihood: {beam_likelihood:.2%}\")\n",
    "\n",
    "# calculate the perplexity and likelihood for the top-k output\n",
    "top_k_perplexity, top_k_likelihood = calculate_perplexity_and_likelihood(top_k_output_scores)\n",
    "print(f\"Top-k perplexity: {top_k_perplexity:.3f}\")\n",
    "print(f\"Top-k likelihood: {top_k_likelihood:.2%}\")\n",
    "\n",
    "# calculate the perplexity and likelihood for the top-p output\n",
    "top_p_perplexity, top_p_likelihood = calculate_perplexity_and_likelihood(top_p_output_scores)\n",
    "print(f\"Top-p perplexity: {top_p_perplexity:.3f}\")\n",
    "print(f\"Top-p likelihood: {top_p_likelihood:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70e0914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f85488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06447440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
