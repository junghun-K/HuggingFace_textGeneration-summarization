{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f140315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCau salLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = \"Today I believe we can finally\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfa00a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = \"It might be possible to\"\n",
    "input_prompt = \"Today I believe we can finally\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9e2f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today I believe we can finally get to the point where we can make a difference in the lives of']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Greedy search\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# set pad_token_id to eos_token_id because GPT2 does not have a PAD token\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# instantiate logits processors\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        MinLengthLogitsProcessor(10, eos_token_id=model.generation_config.eos_token_id),\n",
    "    ]\n",
    ")\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
    "\n",
    "outputs = model.greedy_search(\n",
    "    input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria\n",
    ")\n",
    "\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31edced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:2658: UserWarning: You don't have defined any stopping_criteria, this will likely loop forever\n",
      "  warnings.warn(\"You don't have defined any stopping_criteria, this will likely loop forever\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I believe we can finally finally get to this point where we can finally get to this point where we can finally get to this point where we can finally get to this point where we can finally get to this point where we can finally get to this point where we can finally get to this point where we can finally get to this point where we can finally get to this point where we can finally get to this point where we can finally get to this point where we can finally get to this point where we can finally get to this point where we can finally get to']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Beam search\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    BeamSearchScorer,\n",
    ")\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "\n",
    "encoder_input_str = \"Today I believe we can finally\"\n",
    "encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "# lets run beam search using 3 beams\n",
    "num_beams = 3\n",
    "# define decoder start token ids\n",
    "input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
    "input_ids = input_ids * model.config.decoder_start_token_id\n",
    "\n",
    "# add encoder_outputs to model keyword arguments\n",
    "model_kwargs = {\n",
    "    \"encoder_outputs\": model.get_encoder()(\n",
    "        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True\n",
    "    )\n",
    "}\n",
    "\n",
    "# instantiate beam scorer\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size=1,\n",
    "    num_beams=num_beams,\n",
    "    device=model.device,\n",
    ")\n",
    "\n",
    "# instantiate logits processors\n",
    "logits_processor = LogitsProcessorList(\n",
    "    [\n",
    "        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
    "    ]\n",
    ")\n",
    "\n",
    "outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)\n",
    "\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b862075b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
